{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import main as a\n",
    "import pickle as pkl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tadpole/X.pkl\", \"rb\") as file:\n",
    "    X = pkl.load(file)\n",
    "early = False\n",
    "with open(\"tadpole/y.pkl\", \"rb\") as file:\n",
    "    if early:\n",
    "        y = pkl.load(file)[:,1:]\n",
    "    else:\n",
    "        y = pkl.load(file)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "y = y.flatten()\n",
    "weight = torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, stratify=y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'main' from '/Users/aviadsusman/Documents/Python_Projects/FLA_2/FLA/main.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = a.npDataset(X_train,y_train)\n",
    "test_dataset = a.npDataset(X_test,y_test)\n",
    "val_dataset = a.npDataset(X_val,y_val)\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [160,40,10]\n",
    "attn_heads = 0\n",
    "model = a.FLANN(input_dim=337, hidden_dims=hidden_dims, output_dim=3, attn_heads=attn_heads, activation=nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "metric = a.MacroF1Score(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.7146\n",
      "Epoch 2, Validation Loss: 0.5688\n",
      "Epoch 3, Validation Loss: 0.4947\n",
      "Epoch 4, Validation Loss: 0.5060\n",
      "Epoch 5, Validation Loss: 0.4300\n",
      "Epoch 6, Validation Loss: 0.3755\n",
      "Epoch 7, Validation Loss: 0.2981\n",
      "Epoch 8, Validation Loss: 0.3217\n",
      "Epoch 9, Validation Loss: 0.2932\n",
      "Epoch 10, Validation Loss: 0.3437\n",
      "Epoch 11, Validation Loss: 0.4066\n",
      "Epoch 12, Validation Loss: 0.3132\n",
      "Epoch 13, Validation Loss: 0.4432\n",
      "Epoch 14, Validation Loss: 0.2864\n",
      "Epoch 15, Validation Loss: 0.2529\n",
      "Epoch 16, Validation Loss: 0.4256\n",
      "Epoch 17, Validation Loss: 0.5065\n",
      "Epoch 18, Validation Loss: 0.3385\n",
      "Epoch 19, Validation Loss: 0.3681\n",
      "Epoch 20, Validation Loss: 0.5179\n",
      "Epoch 21, Validation Loss: 0.3981\n",
      "Epoch 22, Validation Loss: 0.4388\n",
      "Epoch 23, Validation Loss: 0.4168\n",
      "Epoch 24, Validation Loss: 0.5389\n",
      "Epoch 25, Validation Loss: 0.5706\n",
      "Early stopping after epoch 25 with validation loss 0.2529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "patience = 10\n",
    "early_stop_counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for inputs, labels in val_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}')\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model = model.state_dict()\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    if early_stop_counter >= patience:\n",
    "        print(f'Early stopping after epoch {epoch+1} with validation loss {best_val_loss:.4f}')\n",
    "        break\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5963, Test Score: 0.8732, Predicted Proba: 0.5508\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        test_loss = criterion(outputs, labels)\n",
    "        test_losses.append(test_loss.item())\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "test_score = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "print(f'Test Loss: {avg_test_loss:.4f}, Test Score: {test_score:.4f}, Predicted Proba: {1/np.exp(avg_test_loss):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.7542\n",
      "Epoch 2, Validation Loss: 0.6166\n",
      "Epoch 3, Validation Loss: 0.5397\n",
      "Epoch 4, Validation Loss: 0.4844\n",
      "Epoch 5, Validation Loss: 0.4243\n",
      "Epoch 6, Validation Loss: 0.4521\n",
      "Epoch 7, Validation Loss: 0.4518\n",
      "Epoch 8, Validation Loss: 0.6154\n",
      "Epoch 9, Validation Loss: 0.4840\n",
      "Epoch 10, Validation Loss: 0.4529\n",
      "Epoch 11, Validation Loss: 0.5213\n",
      "Epoch 12, Validation Loss: 0.4457\n",
      "Epoch 13, Validation Loss: 0.6760\n",
      "Epoch 14, Validation Loss: 0.5944\n",
      "Epoch 15, Validation Loss: 0.5048\n",
      "Early stopping after epoch 15 with validation loss 0.4243\n",
      "Test Loss: 0.4312, Test Score: 0.8475, Predicted Proba: 0.6498\n",
      "Epoch 1, Validation Loss: 0.6755\n",
      "Epoch 2, Validation Loss: 0.5904\n",
      "Epoch 3, Validation Loss: 0.5442\n",
      "Epoch 4, Validation Loss: 0.4970\n",
      "Epoch 5, Validation Loss: 0.4607\n",
      "Epoch 6, Validation Loss: 0.4657\n",
      "Epoch 7, Validation Loss: 0.4921\n",
      "Epoch 8, Validation Loss: 0.5264\n",
      "Epoch 9, Validation Loss: 0.6483\n",
      "Epoch 10, Validation Loss: 0.5127\n",
      "Epoch 11, Validation Loss: 0.3712\n",
      "Epoch 12, Validation Loss: 0.5415\n",
      "Epoch 13, Validation Loss: 0.5697\n",
      "Epoch 14, Validation Loss: 0.4087\n",
      "Epoch 15, Validation Loss: 0.4596\n",
      "Epoch 16, Validation Loss: 0.4555\n",
      "Epoch 17, Validation Loss: 0.5341\n",
      "Epoch 18, Validation Loss: 0.6754\n",
      "Epoch 19, Validation Loss: 0.5273\n",
      "Epoch 20, Validation Loss: 0.4586\n",
      "Epoch 21, Validation Loss: 0.4821\n",
      "Early stopping after epoch 21 with validation loss 0.3712\n",
      "Test Loss: 0.5211, Test Score: 0.8678, Predicted Proba: 0.5939\n",
      "Epoch 1, Validation Loss: 0.7989\n",
      "Epoch 2, Validation Loss: 0.6237\n",
      "Epoch 3, Validation Loss: 0.5209\n",
      "Epoch 4, Validation Loss: 0.5074\n",
      "Epoch 5, Validation Loss: 0.4580\n",
      "Epoch 6, Validation Loss: 0.4729\n",
      "Epoch 7, Validation Loss: 0.4174\n",
      "Epoch 8, Validation Loss: 0.5647\n",
      "Epoch 9, Validation Loss: 0.5089\n",
      "Epoch 10, Validation Loss: 0.4647\n",
      "Epoch 11, Validation Loss: 0.5879\n",
      "Epoch 12, Validation Loss: 0.5590\n",
      "Epoch 13, Validation Loss: 0.6672\n",
      "Epoch 14, Validation Loss: 0.4784\n",
      "Epoch 15, Validation Loss: 0.3470\n",
      "Epoch 16, Validation Loss: 0.6180\n",
      "Epoch 17, Validation Loss: 0.5676\n",
      "Epoch 18, Validation Loss: 0.6827\n",
      "Epoch 19, Validation Loss: 0.8265\n",
      "Epoch 20, Validation Loss: 0.6362\n",
      "Epoch 21, Validation Loss: 1.0492\n",
      "Epoch 22, Validation Loss: 0.7668\n",
      "Epoch 23, Validation Loss: 0.7282\n",
      "Epoch 24, Validation Loss: 0.6716\n",
      "Epoch 25, Validation Loss: 0.4961\n",
      "Early stopping after epoch 25 with validation loss 0.3470\n",
      "Test Loss: 0.5681, Test Score: 0.8203, Predicted Proba: 0.5666\n",
      "Epoch 1, Validation Loss: 0.7776\n",
      "Epoch 2, Validation Loss: 0.5995\n",
      "Epoch 3, Validation Loss: 0.5220\n",
      "Epoch 4, Validation Loss: 0.4676\n",
      "Epoch 5, Validation Loss: 0.5316\n",
      "Epoch 6, Validation Loss: 0.6130\n",
      "Epoch 7, Validation Loss: 0.4794\n",
      "Epoch 8, Validation Loss: 0.4667\n",
      "Epoch 9, Validation Loss: 0.4058\n",
      "Epoch 10, Validation Loss: 0.3946\n",
      "Epoch 11, Validation Loss: 0.4330\n",
      "Epoch 12, Validation Loss: 0.4656\n",
      "Epoch 13, Validation Loss: 0.6335\n",
      "Epoch 14, Validation Loss: 0.4086\n",
      "Epoch 15, Validation Loss: 0.4559\n",
      "Epoch 16, Validation Loss: 0.6500\n",
      "Epoch 17, Validation Loss: 0.4902\n",
      "Epoch 18, Validation Loss: 0.4697\n",
      "Epoch 19, Validation Loss: 0.4325\n",
      "Epoch 20, Validation Loss: 0.4898\n",
      "Early stopping after epoch 20 with validation loss 0.3946\n",
      "Test Loss: 0.5198, Test Score: 0.8713, Predicted Proba: 0.5947\n",
      "Epoch 1, Validation Loss: 0.8471\n",
      "Epoch 2, Validation Loss: 0.6960\n",
      "Epoch 3, Validation Loss: 0.5918\n",
      "Epoch 4, Validation Loss: 0.5283\n",
      "Epoch 5, Validation Loss: 0.4303\n",
      "Epoch 6, Validation Loss: 0.4329\n",
      "Epoch 7, Validation Loss: 0.3848\n",
      "Epoch 8, Validation Loss: 0.5724\n",
      "Epoch 9, Validation Loss: 0.4388\n",
      "Epoch 10, Validation Loss: 0.4171\n",
      "Epoch 11, Validation Loss: 0.4724\n",
      "Epoch 12, Validation Loss: 0.4124\n",
      "Epoch 13, Validation Loss: 0.7269\n",
      "Epoch 14, Validation Loss: 0.4459\n",
      "Epoch 15, Validation Loss: 0.6851\n",
      "Epoch 16, Validation Loss: 0.3931\n",
      "Epoch 17, Validation Loss: 0.4114\n",
      "Early stopping after epoch 17 with validation loss 0.3848\n",
      "Test Loss: 0.5539, Test Score: 0.8652, Predicted Proba: 0.5747\n",
      "Epoch 1, Validation Loss: 0.7598\n",
      "Epoch 2, Validation Loss: 0.6380\n",
      "Epoch 3, Validation Loss: 0.6002\n",
      "Epoch 4, Validation Loss: 0.4992\n",
      "Epoch 5, Validation Loss: 0.4420\n",
      "Epoch 6, Validation Loss: 0.4569\n",
      "Epoch 7, Validation Loss: 0.5902\n",
      "Epoch 8, Validation Loss: 0.4442\n",
      "Epoch 9, Validation Loss: 0.3921\n",
      "Epoch 10, Validation Loss: 0.3529\n",
      "Epoch 11, Validation Loss: 0.4275\n",
      "Epoch 12, Validation Loss: 0.5076\n",
      "Epoch 13, Validation Loss: 0.4897\n",
      "Epoch 14, Validation Loss: 0.3778\n",
      "Epoch 15, Validation Loss: 0.5954\n",
      "Epoch 16, Validation Loss: 0.5319\n",
      "Epoch 17, Validation Loss: 0.5968\n",
      "Epoch 18, Validation Loss: 0.5860\n",
      "Epoch 19, Validation Loss: 0.6170\n",
      "Epoch 20, Validation Loss: 0.4811\n",
      "Early stopping after epoch 20 with validation loss 0.3529\n",
      "Test Loss: 0.5254, Test Score: 0.8373, Predicted Proba: 0.5913\n",
      "Epoch 1, Validation Loss: 0.7325\n",
      "Epoch 2, Validation Loss: 0.6228\n",
      "Epoch 3, Validation Loss: 0.5614\n",
      "Epoch 4, Validation Loss: 0.4723\n",
      "Epoch 5, Validation Loss: 0.5556\n",
      "Epoch 6, Validation Loss: 0.4727\n",
      "Epoch 7, Validation Loss: 0.5125\n",
      "Epoch 8, Validation Loss: 0.5045\n",
      "Epoch 9, Validation Loss: 0.4395\n",
      "Epoch 10, Validation Loss: 0.4406\n",
      "Epoch 11, Validation Loss: 0.3932\n",
      "Epoch 12, Validation Loss: 0.6058\n",
      "Epoch 13, Validation Loss: 0.4642\n",
      "Epoch 14, Validation Loss: 0.3861\n",
      "Epoch 15, Validation Loss: 0.6776\n",
      "Epoch 16, Validation Loss: 0.5841\n",
      "Epoch 17, Validation Loss: 0.5125\n",
      "Epoch 18, Validation Loss: 0.5369\n"
     ]
    }
   ],
   "source": [
    "seeds=10\n",
    "heads=15\n",
    "scores = {head: [] for head in range(heads)}\n",
    "probas = {head: [] for head in range(heads)}\n",
    "for seed in range(seeds):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.2, random_state=seed)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, stratify=y_train, test_size=0.1, random_state=seed)\n",
    "    train_dataset = a.npDataset(X_train,y_train)\n",
    "    test_dataset = a.npDataset(X_test,y_test)\n",
    "    val_dataset = a.npDataset(X_val,y_val)\n",
    "    batch_size = 100\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    hidden_dims = [160,40,10]\n",
    "    for head_num in range(heads):\n",
    "        attn_heads = head_num\n",
    "        model = a.FLANN(input_dim=337, hidden_dims=hidden_dims, output_dim=3, attn_heads=attn_heads, activation=nn.ReLU())\n",
    "        criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        metric = a.MacroF1Score(num_classes=3)       \n",
    "\n",
    "        num_epochs = 500\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "        patience = 10\n",
    "        early_stop_counter = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for inputs, labels in val_loader:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss = criterion(outputs, labels)\n",
    "                    val_losses.append(val_loss.item())\n",
    "            \n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            print(f'Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model = model.state_dict()\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "            \n",
    "            if early_stop_counter >= patience:\n",
    "                print(f'Early stopping after epoch {epoch+1} with validation loss {best_val_loss:.4f}')\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(best_model) \n",
    "        test_losses = []\n",
    "        test_predictions = []\n",
    "        test_true_labels = []\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                test_loss = criterion(outputs, labels)\n",
    "                test_losses.append(test_loss.item())\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "                test_predictions.extend(predictions.cpu().numpy())\n",
    "                test_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_test_loss = np.mean(test_losses)\n",
    "        test_score = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "        print(f'Test Loss: {avg_test_loss:.4f}, Test Score: {test_score:.4f}, Predicted Proba: {1/np.exp(avg_test_loss):.4f}')\n",
    "        scores[head_num].append(test_score)\n",
    "        probas[head_num].append(1/np.exp(avg_test_loss))\n",
    "\n",
    "scores = pd.DataFrame(scores)\n",
    "probas = pd.DataFrame(probas)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
