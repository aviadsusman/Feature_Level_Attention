{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import main as a\n",
    "from datasets import load_diabetes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle as pkl\n",
    "from importlib import reload\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([data['X_train'][k] for k in data['X_train'].keys()], axis=1)\n",
    "X_test = pd.concat([data['X_test'][k] for k in data['X_test'].keys()], axis=1)\n",
    "X_raw = pd.concat([X_train, X_test], axis=0).to_numpy()\n",
    "y = pd.concat([data['y_train'], data['y_test']], axis=0).to_numpy().flatten()\n",
    "y_counts = np.unique(y, return_counts=True)[1]\n",
    "weight = torch.tensor([y_counts[0]/y_counts[1]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_imputed_not_norm = imputer.fit_transform(X_raw)\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X_imputed_not_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'main' from '/Users/aviadsusman/Documents/Python_Projects/FeatureLevelAttention/FLA/main.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 1, with 20 heads\n",
      "Epoch 1, Validation Loss: 1.1861\n",
      "Epoch 2, Validation Loss: 1.1574\n",
      "Epoch 3, Validation Loss: 1.1600\n",
      "Epoch 4, Validation Loss: 1.1715\n",
      "Epoch 5, Validation Loss: 1.1081\n",
      "Epoch 6, Validation Loss: 1.1219\n",
      "Epoch 7, Validation Loss: 1.1244\n",
      "Epoch 8, Validation Loss: 1.1440\n",
      "Epoch 9, Validation Loss: 1.1421\n",
      "Epoch 10, Validation Loss: 1.1409\n",
      "Epoch 11, Validation Loss: 1.1387\n",
      "Epoch 12, Validation Loss: 1.1188\n",
      "Epoch 13, Validation Loss: 1.1411\n",
      "Epoch 14, Validation Loss: 1.1722\n",
      "Epoch 15, Validation Loss: 1.1230\n",
      "Early stopping after epoch 15 with validation loss 1.1081\n",
      "Test Loss: 1.1660, Test Score: 0.2598 for seed 1 and 20 heads.\n",
      "seed 1, with 25 heads\n",
      "Epoch 1, Validation Loss: 1.1827\n",
      "Epoch 2, Validation Loss: 1.1623\n",
      "Epoch 3, Validation Loss: 1.1404\n",
      "Epoch 4, Validation Loss: 1.1568\n",
      "Epoch 5, Validation Loss: 1.1270\n",
      "Epoch 6, Validation Loss: 1.1219\n",
      "Epoch 7, Validation Loss: 1.1541\n",
      "Epoch 8, Validation Loss: 1.1211\n",
      "Epoch 9, Validation Loss: 1.1059\n",
      "Epoch 10, Validation Loss: 1.1195\n",
      "Epoch 11, Validation Loss: 1.1176\n",
      "Epoch 12, Validation Loss: 1.1155\n",
      "Epoch 13, Validation Loss: 1.1083\n",
      "Epoch 14, Validation Loss: 1.1167\n",
      "Epoch 15, Validation Loss: 1.1105\n",
      "Epoch 16, Validation Loss: 1.1315\n",
      "Epoch 17, Validation Loss: 1.1243\n",
      "Epoch 18, Validation Loss: 1.1101\n",
      "Epoch 19, Validation Loss: 1.1059\n",
      "Early stopping after epoch 19 with validation loss 1.1059\n",
      "Test Loss: 1.1584, Test Score: 0.2634 for seed 1 and 25 heads.\n",
      "seed 2, with 20 heads\n",
      "Epoch 1, Validation Loss: 1.1688\n",
      "Epoch 2, Validation Loss: 1.1748\n",
      "Epoch 3, Validation Loss: 1.1699\n",
      "Epoch 4, Validation Loss: 1.1598\n",
      "Epoch 5, Validation Loss: 1.1465\n",
      "Epoch 6, Validation Loss: 1.1344\n",
      "Epoch 7, Validation Loss: 1.1454\n",
      "Epoch 8, Validation Loss: 1.1417\n",
      "Epoch 9, Validation Loss: 1.1397\n",
      "Epoch 10, Validation Loss: 1.1275\n",
      "Epoch 11, Validation Loss: 1.1345\n",
      "Epoch 12, Validation Loss: 1.1363\n",
      "Epoch 13, Validation Loss: 1.1308\n",
      "Epoch 14, Validation Loss: 1.1572\n",
      "Epoch 15, Validation Loss: 1.1235\n",
      "Epoch 16, Validation Loss: 1.1306\n",
      "Epoch 17, Validation Loss: 1.1168\n",
      "Epoch 18, Validation Loss: 1.1325\n",
      "Epoch 19, Validation Loss: 1.1260\n",
      "Epoch 20, Validation Loss: 1.1368\n",
      "Epoch 21, Validation Loss: 1.1337\n",
      "Epoch 22, Validation Loss: 1.1254\n",
      "Epoch 23, Validation Loss: 1.1232\n",
      "Epoch 24, Validation Loss: 1.1401\n",
      "Epoch 25, Validation Loss: 1.1212\n",
      "Epoch 26, Validation Loss: 1.1423\n",
      "Epoch 27, Validation Loss: 1.1329\n",
      "Early stopping after epoch 27 with validation loss 1.1168\n",
      "Test Loss: 1.1658, Test Score: 0.2742 for seed 2 and 20 heads.\n",
      "seed 2, with 25 heads\n",
      "Epoch 1, Validation Loss: 1.1952\n",
      "Epoch 2, Validation Loss: 1.1908\n",
      "Epoch 3, Validation Loss: 1.1718\n",
      "Epoch 4, Validation Loss: 1.1566\n",
      "Epoch 5, Validation Loss: 1.1470\n",
      "Epoch 6, Validation Loss: 1.1364\n",
      "Epoch 7, Validation Loss: 1.1294\n",
      "Epoch 8, Validation Loss: 1.1461\n",
      "Epoch 9, Validation Loss: 1.1369\n",
      "Epoch 10, Validation Loss: 1.1320\n",
      "Epoch 11, Validation Loss: 1.1476\n",
      "Epoch 12, Validation Loss: 1.1242\n",
      "Epoch 13, Validation Loss: 1.1519\n",
      "Epoch 14, Validation Loss: 1.1434\n",
      "Epoch 15, Validation Loss: 1.1381\n",
      "Epoch 16, Validation Loss: 1.1341\n",
      "Epoch 17, Validation Loss: 1.1479\n",
      "Epoch 18, Validation Loss: 1.1535\n",
      "Epoch 19, Validation Loss: 1.1340\n",
      "Epoch 20, Validation Loss: 1.1261\n",
      "Epoch 21, Validation Loss: 1.1353\n",
      "Epoch 22, Validation Loss: 1.1342\n",
      "Early stopping after epoch 22 with validation loss 1.1242\n",
      "Test Loss: 1.1557, Test Score: 0.2685 for seed 2 and 25 heads.\n",
      "seed 3, with 20 heads\n",
      "Epoch 1, Validation Loss: 1.1445\n",
      "Epoch 2, Validation Loss: 1.1562\n",
      "Epoch 3, Validation Loss: 1.1377\n",
      "Epoch 4, Validation Loss: 1.1351\n",
      "Epoch 5, Validation Loss: 1.1192\n",
      "Epoch 6, Validation Loss: 1.1246\n",
      "Epoch 7, Validation Loss: 1.1086\n",
      "Epoch 8, Validation Loss: 1.1067\n",
      "Epoch 9, Validation Loss: 1.1087\n",
      "Epoch 10, Validation Loss: 1.1082\n",
      "Epoch 11, Validation Loss: 1.1171\n",
      "Epoch 12, Validation Loss: 1.1174\n",
      "Epoch 13, Validation Loss: 1.1144\n",
      "Epoch 14, Validation Loss: 1.1221\n",
      "Epoch 15, Validation Loss: 1.0973\n",
      "Epoch 16, Validation Loss: 1.1282\n",
      "Epoch 17, Validation Loss: 1.0983\n",
      "Epoch 18, Validation Loss: 1.1142\n",
      "Epoch 19, Validation Loss: 1.1009\n",
      "Epoch 20, Validation Loss: 1.1252\n",
      "Epoch 21, Validation Loss: 1.1086\n",
      "Epoch 22, Validation Loss: 1.1224\n",
      "Epoch 23, Validation Loss: 1.1213\n",
      "Epoch 24, Validation Loss: 1.1349\n",
      "Epoch 25, Validation Loss: 1.1515\n",
      "Early stopping after epoch 25 with validation loss 1.0973\n",
      "Test Loss: 1.1574, Test Score: 0.2796 for seed 3 and 20 heads.\n",
      "seed 3, with 25 heads\n",
      "Epoch 1, Validation Loss: 1.1775\n",
      "Epoch 2, Validation Loss: 1.1578\n",
      "Epoch 3, Validation Loss: 1.1644\n",
      "Epoch 4, Validation Loss: 1.1453\n",
      "Epoch 5, Validation Loss: 1.1450\n",
      "Epoch 6, Validation Loss: 1.1408\n",
      "Epoch 7, Validation Loss: 1.1409\n",
      "Epoch 8, Validation Loss: 1.1309\n",
      "Epoch 9, Validation Loss: 1.1416\n",
      "Epoch 10, Validation Loss: 1.1333\n",
      "Epoch 11, Validation Loss: 1.1241\n",
      "Epoch 12, Validation Loss: 1.1187\n",
      "Epoch 13, Validation Loss: 1.1182\n",
      "Epoch 14, Validation Loss: 1.1231\n",
      "Epoch 15, Validation Loss: 1.1115\n",
      "Epoch 16, Validation Loss: 1.1117\n",
      "Epoch 17, Validation Loss: 1.1269\n",
      "Epoch 18, Validation Loss: 1.1336\n",
      "Epoch 19, Validation Loss: 1.1344\n",
      "Epoch 20, Validation Loss: 1.1425\n",
      "Epoch 21, Validation Loss: 1.1416\n",
      "Epoch 22, Validation Loss: 1.1419\n",
      "Epoch 23, Validation Loss: 1.1263\n",
      "Epoch 24, Validation Loss: 1.1244\n",
      "Epoch 25, Validation Loss: 1.1389\n",
      "Early stopping after epoch 25 with validation loss 1.1115\n",
      "Test Loss: 1.1395, Test Score: 0.2965 for seed 3 and 25 heads.\n",
      "seed 4, with 20 heads\n",
      "Epoch 1, Validation Loss: 1.1952\n",
      "Epoch 2, Validation Loss: 1.1903\n",
      "Epoch 3, Validation Loss: 1.1521\n",
      "Epoch 4, Validation Loss: 1.1601\n",
      "Epoch 5, Validation Loss: 1.1419\n",
      "Epoch 6, Validation Loss: 1.1468\n",
      "Epoch 7, Validation Loss: 1.1679\n",
      "Epoch 8, Validation Loss: 1.1495\n",
      "Epoch 9, Validation Loss: 1.1376\n",
      "Epoch 10, Validation Loss: 1.1512\n",
      "Epoch 11, Validation Loss: 1.1426\n",
      "Epoch 12, Validation Loss: 1.1528\n",
      "Epoch 13, Validation Loss: 1.1564\n",
      "Epoch 14, Validation Loss: 1.1414\n",
      "Epoch 15, Validation Loss: 1.1427\n",
      "Epoch 16, Validation Loss: 1.1444\n",
      "Epoch 17, Validation Loss: 1.1402\n",
      "Epoch 18, Validation Loss: 1.1289\n",
      "Epoch 19, Validation Loss: 1.1373\n",
      "Epoch 20, Validation Loss: 1.1276\n",
      "Epoch 21, Validation Loss: 1.1478\n",
      "Epoch 22, Validation Loss: 1.1509\n",
      "Epoch 23, Validation Loss: 1.1415\n",
      "Epoch 24, Validation Loss: 1.1341\n",
      "Epoch 25, Validation Loss: 1.1448\n",
      "Epoch 26, Validation Loss: 1.1451\n",
      "Epoch 27, Validation Loss: 1.1489\n",
      "Epoch 28, Validation Loss: 1.1374\n",
      "Epoch 29, Validation Loss: 1.1332\n",
      "Epoch 30, Validation Loss: 1.1433\n",
      "Early stopping after epoch 30 with validation loss 1.1276\n",
      "Test Loss: 1.1297, Test Score: 0.2715 for seed 4 and 20 heads.\n",
      "seed 4, with 25 heads\n",
      "Epoch 1, Validation Loss: 1.1815\n",
      "Epoch 2, Validation Loss: 1.1581\n",
      "Epoch 3, Validation Loss: 1.1496\n",
      "Epoch 4, Validation Loss: 1.1478\n",
      "Epoch 5, Validation Loss: 1.1669\n",
      "Epoch 6, Validation Loss: 1.1600\n",
      "Epoch 7, Validation Loss: 1.1674\n",
      "Epoch 8, Validation Loss: 1.1458\n",
      "Epoch 9, Validation Loss: 1.1457\n",
      "Epoch 10, Validation Loss: 1.1395\n",
      "Epoch 11, Validation Loss: 1.1488\n",
      "Epoch 12, Validation Loss: 1.1451\n",
      "Epoch 13, Validation Loss: 1.1454\n",
      "Epoch 14, Validation Loss: 1.1416\n",
      "Epoch 15, Validation Loss: 1.1625\n",
      "Epoch 16, Validation Loss: 1.1437\n",
      "Epoch 17, Validation Loss: 1.1417\n",
      "Epoch 18, Validation Loss: 1.1589\n",
      "Epoch 19, Validation Loss: 1.1595\n",
      "Epoch 20, Validation Loss: 1.1478\n",
      "Early stopping after epoch 20 with validation loss 1.1395\n",
      "Test Loss: 1.1330, Test Score: 0.2946 for seed 4 and 25 heads.\n",
      "seed 5, with 20 heads\n",
      "Epoch 1, Validation Loss: 1.1646\n",
      "Epoch 2, Validation Loss: 1.1665\n",
      "Epoch 3, Validation Loss: 1.1612\n",
      "Epoch 4, Validation Loss: 1.1427\n",
      "Epoch 5, Validation Loss: 1.1430\n",
      "Epoch 6, Validation Loss: 1.1272\n",
      "Epoch 7, Validation Loss: 1.1216\n",
      "Epoch 8, Validation Loss: 1.1225\n",
      "Epoch 9, Validation Loss: 1.1615\n",
      "Epoch 10, Validation Loss: 1.1167\n",
      "Epoch 11, Validation Loss: 1.1229\n",
      "Epoch 12, Validation Loss: 1.1236\n",
      "Epoch 13, Validation Loss: 1.1032\n",
      "Epoch 14, Validation Loss: 1.1112\n",
      "Epoch 15, Validation Loss: 1.0918\n",
      "Epoch 16, Validation Loss: 1.1005\n",
      "Epoch 17, Validation Loss: 1.0928\n",
      "Epoch 18, Validation Loss: 1.0912\n",
      "Epoch 19, Validation Loss: 1.0893\n",
      "Epoch 20, Validation Loss: 1.1101\n",
      "Epoch 21, Validation Loss: 1.0996\n",
      "Epoch 22, Validation Loss: 1.0930\n",
      "Epoch 23, Validation Loss: 1.1034\n",
      "Epoch 24, Validation Loss: 1.0951\n",
      "Epoch 25, Validation Loss: 1.0956\n",
      "Epoch 26, Validation Loss: 1.1016\n",
      "Epoch 27, Validation Loss: 1.0888\n",
      "Epoch 28, Validation Loss: 1.0955\n",
      "Epoch 29, Validation Loss: 1.0890\n",
      "Epoch 30, Validation Loss: 1.0992\n",
      "Epoch 31, Validation Loss: 1.0850\n",
      "Epoch 32, Validation Loss: 1.0891\n",
      "Epoch 33, Validation Loss: 1.1101\n",
      "Epoch 34, Validation Loss: 1.0717\n",
      "Epoch 35, Validation Loss: 1.0806\n",
      "Epoch 36, Validation Loss: 1.0846\n",
      "Epoch 37, Validation Loss: 1.0887\n",
      "Epoch 38, Validation Loss: 1.0823\n",
      "Epoch 39, Validation Loss: 1.0916\n",
      "Epoch 40, Validation Loss: 1.0902\n",
      "Epoch 41, Validation Loss: 1.0981\n",
      "Epoch 42, Validation Loss: 1.0862\n",
      "Epoch 43, Validation Loss: 1.0961\n",
      "Epoch 44, Validation Loss: 1.1078\n",
      "Early stopping after epoch 44 with validation loss 1.0717\n",
      "Test Loss: 1.1981, Test Score: 0.2362 for seed 5 and 20 heads.\n",
      "seed 5, with 25 heads\n",
      "Epoch 1, Validation Loss: 1.1383\n",
      "Epoch 2, Validation Loss: 1.1387\n",
      "Epoch 3, Validation Loss: 1.1352\n",
      "Epoch 4, Validation Loss: 1.1367\n",
      "Epoch 5, Validation Loss: 1.1258\n",
      "Epoch 6, Validation Loss: 1.1556\n",
      "Epoch 7, Validation Loss: 1.1258\n",
      "Epoch 8, Validation Loss: 1.1215\n",
      "Epoch 9, Validation Loss: 1.1088\n",
      "Epoch 10, Validation Loss: 1.1204\n",
      "Epoch 11, Validation Loss: 1.1036\n",
      "Epoch 12, Validation Loss: 1.1101\n",
      "Epoch 13, Validation Loss: 1.1048\n",
      "Epoch 14, Validation Loss: 1.1080\n",
      "Epoch 15, Validation Loss: 1.1179\n",
      "Epoch 16, Validation Loss: 1.1120\n",
      "Epoch 17, Validation Loss: 1.1307\n",
      "Epoch 18, Validation Loss: 1.1215\n",
      "Epoch 19, Validation Loss: 1.1208\n",
      "Epoch 20, Validation Loss: 1.1085\n",
      "Epoch 21, Validation Loss: 1.1152\n",
      "Early stopping after epoch 21 with validation loss 1.1036\n",
      "Test Loss: 1.1807, Test Score: 0.2037 for seed 5 and 25 heads.\n",
      "seed 6, with 20 heads\n",
      "Epoch 1, Validation Loss: 1.1828\n",
      "Epoch 2, Validation Loss: 1.1514\n",
      "Epoch 3, Validation Loss: 1.1358\n",
      "Epoch 4, Validation Loss: 1.1175\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "head_counts = [20,25]#[0,5,10,15]\n",
    "test_prediction_dict = {h: [] for h in head_counts}\n",
    "test_label_list = []\n",
    "losses = {h: [] for h in head_counts}\n",
    "for seed in range(10):\n",
    "    for head in head_counts:\n",
    "        print(f'seed {seed+1}, with {head} heads')\n",
    "        #split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.2, random_state=seed)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, stratify=y_train, test_size=0.1, random_state=seed)\n",
    "        train_dataset = a.npDataset(X_train,y_train)\n",
    "        test_dataset = a.npDataset(X_test,y_test)\n",
    "        val_dataset = a.npDataset(X_val,y_val)\n",
    "        batch_size = 100\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        #make model\n",
    "        hidden_dims = [50,25,10]\n",
    "        attn_heads = head\n",
    "        model = a.FLANN(input_dim=108, hidden_dims=hidden_dims, output_dim=1, attn_heads=attn_heads, activation=nn.ReLU())\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        #train\n",
    "        num_epochs = 500\n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "        patience = 10\n",
    "        early_stop_counter = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                labels = labels.unsqueeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for inputs, labels in val_loader:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    labels = labels.unsqueeze(1)\n",
    "                    val_loss = criterion(outputs, labels)\n",
    "                    val_losses.append(val_loss.item())\n",
    "            \n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            print(f'Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model = model.state_dict()\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "            \n",
    "            if early_stop_counter >= patience:\n",
    "                print(f'Early stopping after epoch {epoch+1} with validation loss {best_val_loss:.4f}')\n",
    "                break\n",
    "            \n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "        #eval\n",
    "        test_losses = []\n",
    "        test_predictions = []\n",
    "        test_true_labels = []\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                labels = labels.unsqueeze(1)\n",
    "                test_loss = criterion(outputs, labels)\n",
    "                test_losses.append(test_loss.item())\n",
    "                test_predictions.extend(outputs.cpu().numpy())\n",
    "                test_true_labels.extend(labels.cpu().numpy())\n",
    "        avg_test_loss = np.mean(test_losses)\n",
    "        test_predictions_f1 = [y>0.5 for y in test_predictions]\n",
    "        test_score = f1_score(test_true_labels, test_predictions_f1)\n",
    "        print(f'Test Loss: {avg_test_loss:.4f}, Test Score: {test_score:.4f} for seed {seed+1} and {head} heads.')\n",
    "        if head == 0:\n",
    "            test_label_list.append(test_true_labels)\n",
    "        test_prediction_dict[head].append(test_predictions)\n",
    "        losses[head].append(avg_test_loss)\n",
    "with open (\"/Users/aviadsusman/Documents/Python_Projects/FeatureLevelAttention/FLA/results/diabetes/test_pred_dict_20s.pkl\", \"wb\") as file:\n",
    "    pkl.dump(test_prediction_dict, file=file)\n",
    "with open (\"/Users/aviadsusman/Documents/Python_Projects/FeatureLevelAttention/FLA/results/diabetes/test_losses_dict_20s.pkl\", \"wb\") as file:\n",
    "    pkl.dump(losses, file=file)\n",
    "with open (\"/Users/aviadsusman/Documents/Python_Projects/FeatureLevelAttention/FLA/results/diabetes/test_labels.pkl\", \"wb\") as file:\n",
    "    pkl.dump(test_label_list, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"/Users/aviadsusman/Documents/Python_Projects/FeatureLevelAttention/FLA/results/diabetes/test_pred_dict_20s.pkl\", \"wb\") as file:\n",
    "    pkl.dump(test_prediction_dict, file=file)\n",
    "with open (\"/Users/aviadsusman/Documents/Python_Projects/FeatureLevelAttention/FLA/results/diabetes/test_losses_dict_20s.pkl\", \"wb\") as file:\n",
    "    pkl.dump(losses, file=file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
