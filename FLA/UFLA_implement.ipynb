{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fla as a\n",
    "import pickle as pkl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MM_Attention/tadpole/X.pkl/tadpole/X.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMM_Attention/tadpole/X.pkl/tadpole/X.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      2\u001b[0m     X \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      3\u001b[0m early \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/MM_Attention/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MM_Attention/tadpole/X.pkl/tadpole/X.pkl'"
     ]
    }
   ],
   "source": [
    "with open(\"MM_Attention/tadpole/X.pkl/tadpole/X.pkl\", \"rb\") as file:\n",
    "    X = pkl.load(file)\n",
    "early = 0\n",
    "with open(\"MM_Attention/tadpole/y.pkl\", \"rb\") as file:\n",
    "    if early:\n",
    "        y = pkl.load(file)[:,1:]\n",
    "    else:\n",
    "        y = pkl.load(file)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "y = y.flatten()\n",
    "weight = torch.tensor(compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, stratify=y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'main' from '/Users/susmaa01/Documents/MM_Attention/main.py'>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = a.npDataset(X_train,y_train)\n",
    "test_dataset = a.npDataset(X_test,y_test)\n",
    "val_dataset = a.npDataset(X_val,y_val)\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dims = [160,40,10]\n",
    "attn_heads = 0\n",
    "model = a.UTAMLP(input_dim=337, attn_heads=attn_heads, hidden_dims=hidden_dims, output_dim=3, activation=nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "metric = a.MacroF1Score(num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.7419\n",
      "Epoch 2, Validation Loss: 0.5934\n",
      "Epoch 3, Validation Loss: 0.4548\n",
      "Epoch 4, Validation Loss: 0.3967\n",
      "Epoch 5, Validation Loss: 0.4412\n",
      "Epoch 6, Validation Loss: 0.3669\n",
      "Epoch 7, Validation Loss: 0.3264\n",
      "Epoch 8, Validation Loss: 0.3368\n",
      "Epoch 9, Validation Loss: 0.3349\n",
      "Epoch 10, Validation Loss: 0.3158\n",
      "Epoch 11, Validation Loss: 0.3152\n",
      "Epoch 12, Validation Loss: 0.3266\n",
      "Epoch 13, Validation Loss: 0.3158\n",
      "Epoch 14, Validation Loss: 0.3272\n",
      "Epoch 15, Validation Loss: 0.3757\n",
      "Epoch 16, Validation Loss: 0.2866\n",
      "Epoch 17, Validation Loss: 0.3081\n",
      "Epoch 18, Validation Loss: 0.3800\n",
      "Epoch 19, Validation Loss: 0.3638\n",
      "Epoch 20, Validation Loss: 0.3631\n",
      "Epoch 21, Validation Loss: 0.4641\n",
      "Epoch 22, Validation Loss: 0.4258\n",
      "Epoch 23, Validation Loss: 0.4560\n",
      "Epoch 24, Validation Loss: 0.4089\n",
      "Epoch 25, Validation Loss: 0.3963\n",
      "Epoch 26, Validation Loss: 0.4275\n",
      "Early stopping after epoch 26 with validation loss 0.2866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UTAMLP(\n",
       "  (utab_attn): UTabAttn(\n",
       "    (weights): ParameterDict()\n",
       "    (biases): ParameterDict()\n",
       "  )\n",
       "  (attn_norm): LayerNorm((337,), eps=1e-05, elementwise_affine=True)\n",
       "  (linears): ModuleList(\n",
       "    (0): Linear(in_features=337, out_features=160, bias=True)\n",
       "    (1): Linear(in_features=160, out_features=40, bias=True)\n",
       "    (2): Linear(in_features=40, out_features=10, bias=True)\n",
       "  )\n",
       "  (linear_norms): ModuleList(\n",
       "    (0): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (activation): ReLU()\n",
       "  (output): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "patience = 10\n",
    "early_stop_counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for inputs, labels in val_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}')\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model = model.state_dict()\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    if early_stop_counter >= patience:\n",
    "        print(f'Early stopping after epoch {epoch+1} with validation loss {best_val_loss:.4f}')\n",
    "        break\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4793, Test Score: 0.8387\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        test_loss = criterion(outputs, labels)\n",
    "        test_losses.append(test_loss.item())\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "test_score = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "print(f'Test Loss: {avg_test_loss:.4f}, Test Score: {test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 0.7355\n",
      "Epoch 2, Validation Loss: 0.5865\n",
      "Epoch 3, Validation Loss: 0.4655\n",
      "Epoch 4, Validation Loss: 0.4020\n",
      "Epoch 5, Validation Loss: 0.3527\n",
      "Epoch 6, Validation Loss: 0.3202\n",
      "Epoch 7, Validation Loss: 0.3045\n",
      "Epoch 8, Validation Loss: 0.3549\n",
      "Epoch 9, Validation Loss: 0.3623\n",
      "Epoch 10, Validation Loss: 0.3074\n",
      "Epoch 11, Validation Loss: 0.4488\n",
      "Epoch 12, Validation Loss: 0.4266\n",
      "Epoch 13, Validation Loss: 0.3810\n",
      "Epoch 14, Validation Loss: 0.3587\n",
      "Epoch 15, Validation Loss: 0.3842\n",
      "Epoch 16, Validation Loss: 0.5750\n",
      "Epoch 17, Validation Loss: 0.4126\n",
      "Early stopping after epoch 17 with validation loss 0.3045\n",
      "Test Loss: 0.3866, Test Score: 0.8113\n",
      "[0, 0.8112513424167015]\n",
      "Epoch 1, Validation Loss: 0.6514\n",
      "Epoch 2, Validation Loss: 0.5696\n",
      "Epoch 3, Validation Loss: 0.4658\n",
      "Epoch 4, Validation Loss: 0.4421\n",
      "Epoch 5, Validation Loss: 0.3649\n",
      "Epoch 6, Validation Loss: 0.3442\n",
      "Epoch 7, Validation Loss: 0.3443\n",
      "Epoch 8, Validation Loss: 0.3560\n",
      "Epoch 9, Validation Loss: 0.3653\n",
      "Epoch 10, Validation Loss: 0.3876\n",
      "Epoch 11, Validation Loss: 0.3950\n",
      "Epoch 12, Validation Loss: 0.4363\n",
      "Epoch 13, Validation Loss: 0.3693\n",
      "Epoch 14, Validation Loss: 0.3507\n",
      "Epoch 15, Validation Loss: 0.4968\n",
      "Epoch 16, Validation Loss: 0.4035\n",
      "Early stopping after epoch 16 with validation loss 0.3442\n",
      "Test Loss: 0.3984, Test Score: 0.8360\n",
      "[1, 0.8360252548893522]\n",
      "Epoch 1, Validation Loss: 0.6534\n",
      "Epoch 2, Validation Loss: 0.5070\n",
      "Epoch 3, Validation Loss: 0.4238\n",
      "Epoch 4, Validation Loss: 0.3788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/MM_Attention/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MM_Attention/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MM_Attention/main.py:84\u001b[0m, in \u001b[0;36mUTAMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 84\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutab_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m     86\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_norm(x)\n",
      "File \u001b[0;32m~/Documents/MM_Attention/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MM_Attention/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MM_Attention/main.py:57\u001b[0m, in \u001b[0;36mUTabAttn.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m key \u001b[38;5;241m=\u001b[39m head_representations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     55\u001b[0m value \u001b[38;5;241m=\u001b[39m head_representations[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 57\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m attended_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(similarity_matrix, value\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     60\u001b[0m representations\u001b[38;5;241m.\u001b[39mappend(attended_value)\n",
      "File \u001b[0;32m~/Documents/MM_Attention/main.py:38\u001b[0m, in \u001b[0;36mUTabAttn.compute_sim\u001b[0;34m(self, x, y, epsilon)\u001b[0m\n\u001b[1;32m     36\u001b[0m diff_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(x \u001b[38;5;241m-\u001b[39m y)\n\u001b[1;32m     37\u001b[0m diff_matrix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m epsilon\n\u001b[0;32m---> 38\u001b[0m sim_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdiff_matrix\u001b[49m\n\u001b[1;32m     40\u001b[0m sim_matrix \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(sim_matrix, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sim_matrix\n",
      "File \u001b[0;32m~/Documents/MM_Attention/.venv/lib/python3.11/site-packages/torch/_tensor.py:34\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m(f):\n\u001b[1;32m     32\u001b[0m     assigned \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mWRAPPER_ASSIGNMENTS\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f, assigned\u001b[38;5;241m=\u001b[39massigned)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[1;32m     38\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = None\n",
    "no_attn_score = None\n",
    "for h in range(15):\n",
    "    hidden_dims = [160,40,10]\n",
    "    attn_heads = h\n",
    "    model = a.UTAMLP(input_dim=337, attn_heads=attn_heads, hidden_dims=hidden_dims, output_dim=3, activation=nn.ReLU())\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    metric = a.MacroF1Score(num_classes=3)\n",
    "\n",
    "    num_epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience = 10\n",
    "    early_stop_counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        for inputs, labels in val_loader:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        print(f'Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}')\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        if early_stop_counter >= patience:\n",
    "            print(f'Early stopping after epoch {epoch+1} with validation loss {best_val_loss:.4f}')\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    test_losses = []\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            test_loss = criterion(outputs, labels)\n",
    "            test_losses.append(test_loss.item())\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            test_predictions.extend(predictions.cpu().numpy())\n",
    "            test_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    test_score = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Score: {test_score:.4f}')\n",
    "    if h==0:\n",
    "        no_attn_score = test_score\n",
    "        best_score = [0, test_score]\n",
    "    else:\n",
    "        if test_score > best_score[1]:\n",
    "            best_score = [h, test_score]\n",
    "    print(best_score)\n",
    "no_attn_score, best_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
